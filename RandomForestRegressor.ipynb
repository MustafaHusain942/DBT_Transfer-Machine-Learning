{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfcbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor for DBT Transfer Prediction\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96585fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../clean_dbt_district_wise.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbe565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Handle missing values if any\n",
    "df = df.dropna()\n",
    "\n",
    "# Create feature matrix and target variable\n",
    "# Features: state, district, year, transactions\n",
    "# Target: total_dbt_transfer\n",
    "\n",
    "# Encode categorical variables\n",
    "le_state = LabelEncoder()\n",
    "le_district = LabelEncoder()\n",
    "\n",
    "# Create a copy for encoding\n",
    "df_encoded = df.copy()\n",
    "df_encoded['state_encoded'] = le_state.fit_transform(df['state_name'])\n",
    "df_encoded['district_encoded'] = le_district.fit_transform(df['district_name'])\n",
    "\n",
    "# Extract year from fy column\n",
    "df_encoded['year'] = df_encoded['start_year']\n",
    "\n",
    "# Prepare features and target\n",
    "feature_columns = ['state_encoded', 'district_encoded', 'year', 'no_of_dbt_transactions']\n",
    "X = df_encoded[feature_columns]\n",
    "y = df_encoded['total_dbt_transfer']\n",
    "\n",
    "print(f\"\\nFeature columns: {feature_columns}\")\n",
    "print(f\"Target variable: total_dbt_transfer\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Display feature statistics\n",
    "print(\"\\nFeature Statistics:\")\n",
    "print(X.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33400f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(f\"Training target range: {y_train.min():,.0f} to {y_train.max():,.0f}\")\n",
    "print(f\"Test target range: {y_test.min():,.0f} to {y_test.max():,.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9c81e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forest Regressor Model\n",
    "# Initialize the Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,           # Number of trees\n",
    "    max_depth=20,               # Maximum depth of trees\n",
    "    min_samples_split=5,        # Minimum samples to split a node\n",
    "    min_samples_leaf=2,         # Minimum samples in a leaf\n",
    "    random_state=42,            # For reproducibility\n",
    "    n_jobs=-1                   # Use all available cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Random Forest Regressor...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Model training completed!\")\n",
    "print(f\"Number of trees: {rf_model.n_estimators}\")\n",
    "print(f\"Feature importance shape: {rf_model.feature_importances_.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b439aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Feature Importance Plot\n",
    "# Get feature importance scores\n",
    "feature_importance = rf_model.feature_importances_\n",
    "feature_names = ['State', 'District', 'Year', 'Number of Transactions']\n",
    "\n",
    "# Create feature importance plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(len(feature_importance)), feature_importance, \n",
    "               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Feature Importance in Random Forest Model', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Importance Score', fontsize=12)\n",
    "plt.xticks(range(len(feature_names)), feature_names, rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, importance) in enumerate(zip(bars, feature_importance)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{importance:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print feature importance values\n",
    "print(\"Feature Importance Scores:\")\n",
    "for name, importance in zip(feature_names, feature_importance):\n",
    "    print(f\"{name}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fbd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Actual vs Predicted Scatterplot\n",
    "# Create subplots for training and test sets\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training set plot\n",
    "ax1.scatter(y_train, y_train_pred, alpha=0.6, color='#FF6B6B', s=50)\n",
    "ax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "ax1.set_xlabel('Actual Total DBT Transfer', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Total DBT Transfer', fontsize=12)\n",
    "ax1.set_title('Training Set: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Test set plot\n",
    "ax2.scatter(y_test, y_test_pred, alpha=0.6, color='#4ECDC4', s=50)\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax2.set_xlabel('Actual Total DBT Transfer', fontsize=12)\n",
    "ax2.set_ylabel('Predicted Total DBT Transfer', fontsize=12)\n",
    "ax2.set_title('Test Set: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add R² scores to the plots\n",
    "from sklearn.metrics import r2_score\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "ax1.text(0.05, 0.95, f'R² = {train_r2:.3f}', transform=ax1.transAxes, \n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), fontsize=12)\n",
    "ax2.text(0.05, 0.95, f'R² = {test_r2:.3f}', transform=ax2.transAxes, \n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training R² Score: {train_r2:.4f}\")\n",
    "print(f\"Test R² Score: {test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0031ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Residual Plot\n",
    "# Calculate residuals\n",
    "train_residuals = y_train - y_train_pred\n",
    "test_residuals = y_test - y_test_pred\n",
    "\n",
    "# Create residual plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Training set residual plot\n",
    "ax1.scatter(y_train_pred, train_residuals, alpha=0.6, color='#FF6B6B', s=50)\n",
    "ax1.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Predicted Total DBT Transfer', fontsize=12)\n",
    "ax1.set_ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "ax1.set_title('Training Set: Residual Plot', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Test set residual plot\n",
    "ax2.scatter(y_test_pred, test_residuals, alpha=0.6, color='#4ECDC4', s=50)\n",
    "ax2.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_xlabel('Predicted Total DBT Transfer', fontsize=12)\n",
    "ax2.set_ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "ax2.set_title('Test Set: Residual Plot', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add residual statistics\n",
    "train_residual_std = np.std(train_residuals)\n",
    "test_residual_std = np.std(test_residuals)\n",
    "\n",
    "ax1.text(0.05, 0.95, f'Residual Std: {train_residual_std:.0f}', transform=ax1.transAxes, \n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), fontsize=12)\n",
    "ax2.text(0.05, 0.95, f'Residual Std: {test_residual_std:.0f}', transform=ax2.transAxes, \n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training Residual Standard Deviation: {train_residual_std:.2f}\")\n",
    "print(f\"Test Residual Standard Deviation: {test_residual_std:.2f}\")\n",
    "\n",
    "# Check for patterns in residuals\n",
    "print(f\"\\nResidual Analysis:\")\n",
    "print(f\"Training residuals mean: {np.mean(train_residuals):.2f}\")\n",
    "print(f\"Test residuals mean: {np.mean(test_residuals):.2f}\")\n",
    "print(f\"Training residuals range: {np.min(train_residuals):.2f} to {np.max(train_residuals):.2f}\")\n",
    "print(f\"Test residuals range: {np.min(test_residuals):.2f} to {np.max(test_residuals):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0bf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Performance Evaluation\n",
    "# Calculate comprehensive performance metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Training set metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Test set metrics\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Create performance comparison table\n",
    "performance_data = {\n",
    "    'Metric': ['R² Score', 'RMSE', 'MAE', 'MSE'],\n",
    "    'Training': [train_r2, train_rmse, train_mae, train_mse],\n",
    "    'Test': [test_r2, test_rmse, test_mae, test_mse]\n",
    "}\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(performance_df.round(4))\n",
    "\n",
    "# Create a visual comparison of metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(performance_data['Metric']))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, performance_data['Training'], width, label='Training', color='#FF6B6B', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, performance_data['Test'], width, label='Test', color='#4ECDC4', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(performance_data['Metric'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model interpretation\n",
    "print(f\"\\nModel Interpretation:\")\n",
    "print(f\"• The model explains {test_r2:.1%} of the variance in total DBT transfers\")\n",
    "print(f\"• Average prediction error: ₹{test_mae:,.0f}\")\n",
    "print(f\"• Root mean square error: ₹{test_rmse:,.0f}\")\n",
    "if test_r2 > 0.8:\n",
    "    print(\"• Excellent model performance!\")\n",
    "elif test_r2 > 0.6:\n",
    "    print(\"• Good model performance!\")\n",
    "elif test_r2 > 0.4:\n",
    "    print(\"• Moderate model performance\")\n",
    "else:\n",
    "    print(\"• Model needs improvement\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25641b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Additional Analysis: Prediction Examples\n",
    "# Show some example predictions\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get some random samples from test set\n",
    "sample_indices = np.random.choice(len(y_test), 10, replace=False)\n",
    "sample_predictions = pd.DataFrame({\n",
    "    'Actual': y_test.iloc[sample_indices].values,\n",
    "    'Predicted': y_test_pred[sample_indices],\n",
    "    'Error': y_test.iloc[sample_indices].values - y_test_pred[sample_indices],\n",
    "    'Error_Percentage': ((y_test.iloc[sample_indices].values - y_test_pred[sample_indices]) / y_test.iloc[sample_indices].values) * 100\n",
    "})\n",
    "\n",
    "print(sample_predictions.round(2))\n",
    "\n",
    "# Feature importance analysis\n",
    "print(f\"\\nFeature Importance Analysis:\")\n",
    "print(\"=\"*40)\n",
    "for i, (feature, importance) in enumerate(zip(feature_names, feature_importance)):\n",
    "    print(f\"{i+1}. {feature}: {importance:.4f} ({importance*100:.1f}%)\")\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(\"=\"*20)\n",
    "print(f\"• Algorithm: Random Forest Regressor\")\n",
    "print(f\"• Number of trees: {rf_model.n_estimators}\")\n",
    "print(f\"• Max depth: {rf_model.max_depth}\")\n",
    "print(f\"• Features used: {len(feature_columns)}\")\n",
    "print(f\"• Training samples: {len(X_train)}\")\n",
    "print(f\"• Test samples: {len(X_test)}\")\n",
    "print(f\"• Model performance: {'Excellent' if test_r2 > 0.8 else 'Good' if test_r2 > 0.6 else 'Moderate' if test_r2 > 0.4 else 'Needs Improvement'}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
